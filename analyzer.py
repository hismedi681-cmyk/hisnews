import os
import uuid
import json
import datetime
from google.cloud import bigquery
from google.oauth2 import service_account
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
project_id = os.getenv("BQ_PROJECT_ID")
sa_json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
location = os.getenv("GCP_LOCATION", "asia-northeast3") # ì„œìš¸ ë¦¬ì „ ìœ ì§€

# 2. ì¸ì¦ ì„¤ì • (ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)
scopes = ["https://www.googleapis.com/auth/cloud-platform"]
sa_info = json.loads(sa_json_str)
creds = service_account.Credentials.from_service_account_info(sa_info, scopes=scopes)

# 3. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
bq_client = bigquery.Client(credentials=creds, project=project_id)
vertexai.init(project=project_id, location=location, credentials=creds)

# [ì„¤ì • ì •ë³´]
DATASET = "kinetic_field"
RAW_TABLE = "raw_stream_native"
RESULT_TABLE = "fmo_final_analysis"

def clean_json_text(text):
    """Gemini ë‹µë³€ì—ì„œ JSON ë§ˆí¬ë‹¤ìš´ íƒœê·¸(```json) ì œê±° ë° ìˆœìˆ˜ JSON ì¶”ì¶œ"""
    # ```json ... ``` íŒ¨í„´ì„ ì°¾ì•„ ë‚´ë¶€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    match = re.search(r'```json\s+(.*?)\s+```', text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

def analyze_article(article_text):
    """Gemini 2.5 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì  ë™ì—­í•™ ë¶„ì„ ìˆ˜í–‰"""
    model = GenerativeModel("gemini-2.5-flash")
    
    with open("engine_prompt.txt", "r", encoding="utf-8") as f:
        system_instruction = f.read()
    
    config = GenerationConfig(
        temperature=0.1, 
        response_mime_type="application/json"
    )
    
    prompt = f"{system_instruction}\n\n[ê¸°ì‚¬]:\n{article_text}"
    response = model.generate_content(prompt, generation_config=config)
    
    # JSON í…ìŠ¤íŠ¸ ì •ì œ í›„ íŒŒì‹±
    cleaned_json = clean_json_text(response.text)
    return json.loads(cleaned_json)

def insert_result(result, meta):
    """ë¶„ì„ëœ ë§¥ë½ê³¼ ì§€í‘œë¥¼ BigQueryì— ì‚½ì…"""
    table_id = f"{project_id}.{DATASET}.{RESULT_TABLE}"
    
    # ë¸íƒ€ì™€ ìœ„ìƒ ì •ë³´ ì¶”ì¶œ
    physics = result.get('physics_engine', {})
    delta_score = physics.get('module_1_delta', {}).get('kl_divergence', 0.0)
    phase = physics.get('module_3_phase', {}).get('current_phase', 'UNKNOWN')
    
    # ë‚´ëŸ¬í‹°ë¸Œ ì¶”ì¶œ (BQ ìŠ¤í‚¤ë§ˆ í•„ë“œëª…ì— ë§ì¶° primary, counter, synthesisë¡œ ë§¤í•‘)
    narratives = result.get('fmo_output', {}).get('module_5_narratives', {})
    
    row = {
        # 1. ê³ ìœ  ID ìƒì„± (REQUIRED í•„ë“œ ì¶©ì¡±)
        "analysis_id": str(uuid.uuid4()), 
        "title_hash": meta['title_hash'],
        "title": meta.get('title', 'Untitled'),
        "published_at": meta['published_at'].isoformat() if hasattr(meta['published_at'], 'isoformat') else meta['published_at'],
        "observed_at": datetime.datetime.now().isoformat(),
        
        # 2. í•µì‹¬ ì§€í‘œ (í•„í„°ë§ìš©)
        "delta_score": float(delta_score),
        "phase": str(phase),
        
        # 3. ë§¥ë½ ì „ì²´ ë³´ì¡´ (JSON íƒ€ì…)
        "analysis_payload": result, 
        
        # 4. ì „ëµì  ë‚´ëŸ¬í‹°ë¸Œ (RECORD íƒ€ì… ë§¤í•‘)
        "strategic_narrative": {
            "primary": narratives.get("primary_narrative"),
            "counter": narratives.get("counter_narrative"),
            "synthesis": narratives.get("strategic_synthesis")
        }
    }
    
    errors = bq_client.insert_rows_json(table_id, [row])
    return errors

def run_analyzer():
    # ë¯¸ë¶„ì„ ê¸°ì‚¬ ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
    query = f"""
    SELECT article_text, title_hash, published_at, title
    FROM `{project_id}.{DATASET}.{RAW_TABLE}` AS raw
    WHERE NOT EXISTS (
        SELECT 1 FROM `{project_id}.{DATASET}.{RESULT_TABLE}` AS res 
        WHERE res.title_hash = raw.title_hash
    )
    AND article_text IS NOT NULL
    ORDER BY published_at DESC LIMIT 5
    """
    
    rows = bq_client.query(query).result()
    
    for row in rows:
        try:
            print(f"ğŸ§¬ ë¶„ì„ ì¤‘: {row.title[:30]}...")
            analysis_res = analyze_article(row.article_text)
            
            # ë©”íƒ€ë°ì´í„°ì— title ì¶”ê°€
            meta = {
                "title_hash": row.title_hash, 
                "published_at": row.published_at,
                "title": row.title
            }
            errors = insert_result(analysis_res, meta)
            
            if not errors:
                print(f"âœ… ì„±ê³µ: {row.title[:20]}...")
            else:
                print(f"âŒ ì‚½ì… ì—ëŸ¬: {errors}")
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_analyzer()
