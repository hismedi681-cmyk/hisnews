import os
import uuid
import re
import json
import datetime
from google.cloud import bigquery
from google.oauth2 import service_account
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
project_id = os.getenv("BQ_PROJECT_ID")
sa_json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
location = os.getenv("GCP_LOCATION", "asia-northeast3") # ì„œìš¸ ë¦¬ì „ ìœ ì§€

# 2. ì¸ì¦ ì„¤ì • (ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)
scopes = ["https://www.googleapis.com/auth/cloud-platform"]
sa_info = json.loads(sa_json_str)
creds = service_account.Credentials.from_service_account_info(sa_info, scopes=scopes)

# 3. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
bq_client = bigquery.Client(credentials=creds, project=project_id)
vertexai.init(project=project_id, location=location, credentials=creds)

# [ì„¤ì • ì •ë³´]
DATASET = "kinetic_field"
RAW_TABLE = "raw_stream_native"
RESULT_TABLE = "fmo_final_analysis"

def clean_json_text(text):
    """Gemini ë‹µë³€ì—ì„œ ìˆœìˆ˜ JSON ê°ì²´ë§Œ ì¶”ì¶œ (ë” ê²¬ê³ í•œ ë²„ì „)"""
    try:
        # 1. ë§ˆí¬ë‹¤ìš´ íƒœê·¸ ì œê±°
        cleaned = re.sub(r'```json\s?|```', '', text).strip()
        
        # 2. ì²« '{'ì™€ ë§ˆì§€ë§‰ '}' ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ì¡± ì œê±°
        start_idx = cleaned.find('{')
        end_idx = cleaned.rfind('}')
        if start_idx != -1 and end_idx != -1:
            cleaned = cleaned[start_idx:end_idx+1]
        return cleaned
    except Exception:
        return text.strip()

def analyze_article(article_text):
    """Gemini 2.5 Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ì  ë™ì—­í•™ ë¶„ì„ ìˆ˜í–‰"""
    model = GenerativeModel("gemini-2.5-flash")
    
    with open("engine_prompt.txt", "r", encoding="utf-8") as f:
        system_instruction = f.read()
    
    config = GenerationConfig(
        temperature=0.1, 
        response_mime_type="application/json"
    )
    
    prompt = f"{system_instruction}\n\n[ê¸°ì‚¬]:\n{article_text}"
    response = model.generate_content(prompt, generation_config=config)
    
    # JSON í…ìŠ¤íŠ¸ ì •ì œ í›„ íŒŒì‹±
    cleaned_json = clean_json_text(response.text)
    return json.loads(cleaned_json)

def insert_result(result, meta):
    table_id = f"{project_id}.{DATASET}.{RESULT_TABLE}"
    
    physics = result.get('physics_engine', {})
    narratives = result.get('fmo_output', {}).get('module_5_narratives', {})
    
    row = {
        "analysis_id": str(uuid.uuid4()),
        "title_hash": meta['title_hash'],
        "title": meta.get('title', 'Untitled'),
        "published_at": meta['published_at'].isoformat() if hasattr(meta['published_at'], 'isoformat') else meta['published_at'],
        "observed_at": datetime.datetime.now().isoformat(),
        
        "delta_score": float(physics.get('module_1_delta', {}).get('kl_divergence', 0.0)),
        "phase": str(physics.get('module_3_phase', {}).get('current_phase', 'UNKNOWN')),
        
        # [ìˆ˜ì • í¬ì¸íŠ¸] JSON íƒ€ì… ì»¬ëŸ¼ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ dumps ì‚¬ìš©
        "analysis_payload": json.dumps(result, ensure_ascii=False), 
        
        "strategic_narrative": {
            "primary": narratives.get("primary_narrative"),
            "counter": narratives.get("counter_narrative"),
            "synthesis": narratives.get("strategic_synthesis")
        }
    }
    
    # insert_rows_jsonì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í–‰ ë°ì´í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤.
    return bq_client.insert_rows_json(table_id, [row])

def run_analyzer():
    # ë¯¸ë¶„ì„ ê¸°ì‚¬ ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
    query = f"""
    SELECT article_text, title_hash, published_at, title
    FROM `{project_id}.{DATASET}.{RAW_TABLE}` AS raw
    WHERE NOT EXISTS (
        SELECT 1 FROM `{project_id}.{DATASET}.{RESULT_TABLE}` AS res 
        WHERE res.title_hash = raw.title_hash
    )
    AND article_text IS NOT NULL
    ORDER BY published_at DESC LIMIT 50
    """
    
    rows = bq_client.query(query).result()
    
    for row in rows:
        try:
            print(f"ğŸ§¬ ë¶„ì„ ì¤‘: {row.title[:30]}...")
            analysis_res = analyze_article(row.article_text)
            
            # ë©”íƒ€ë°ì´í„°ì— title ì¶”ê°€
            meta = {
                "title_hash": row.title_hash, 
                "published_at": row.published_at,
                "title": row.title
            }
            errors = insert_result(analysis_res, meta)
            
            if not errors:
                print(f"âœ… ì„±ê³µ: {row.title[:20]}...")
            else:
                print(f"âŒ ì‚½ì… ì—ëŸ¬: {errors}")
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_analyzer()
