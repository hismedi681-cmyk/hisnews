import os
import json
import datetime
from google.cloud import bigquery
from google.oauth2 import service_account
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (GitHub Secrets)
project_id = os.getenv("BQ_PROJECT_ID")
sa_json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
location = os.getenv("GCP_LOCATION", "asia-northeast3")

# 2. ì¸ì¦ ì„¤ì • (ë©”ëª¨ë¦¬ ë‚´ì—ì„œ ì²˜ë¦¬)
# ì™¸ë¶€ ì‹œíŠ¸ë¥¼ ì½ì§€ ì•Šìœ¼ë¯€ë¡œ 'cloud-platform' ìŠ¤ì½”í”„ë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.
scopes = ["https://www.googleapis.com/auth/cloud-platform"]
sa_info = json.loads(sa_json_str)
creds = service_account.Credentials.from_service_account_info(sa_info, scopes=scopes)

# 3. ê° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì¸ì¦ ê°ì²´ ì£¼ì…)
# BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
bq_client = bigquery.Client(credentials=creds, project=project_id)

# Vertex AI ì´ˆê¸°í™” (ì¸ì¦ ê°ì²´ ì „ë‹¬)
vertexai.init(project=project_id, location=location, credentials=creds)

# [ì„¤ì • ì •ë³´]
DATASET = "kinetic_field" # reader.py ì˜ˆì‹œì™€ ë§ì¶¤
RAW_TABLE = "raw_stream_native"
RESULT_TABLE = "fmo_final_analysis"

# ------------------------------------------
# ë¶„ì„ ë° ì‚½ì… í•¨ìˆ˜ë“¤ (ì´ì „ê³¼ ë¡œì§ì€ ë™ì¼í•˜ë‚˜ bq_clientë¥¼ ì‚¬ìš©)
# ------------------------------------------

def analyze_article(article_text):
    """ì´ì¤‘ë‚˜ì„  ë™ì—­í•™ ì—”ì§„ ì‹¤í–‰"""
    model = GenerativeModel("gemini-1.5-pro")
    
    # engine_prompt.txt ë¡œë“œ ë¡œì§ì€ ë™ì¼
    with open("engine_prompt.txt", "r", encoding="utf-8") as f:
        system_instruction = f.read()
    
    config = GenerationConfig(temperature=0.1, response_mime_type="application/json")
    prompt = f"{system_instruction}\n\n[ê¸°ì‚¬]:\n{article_text}"
    
    response = model.generate_content(prompt, generation_config=config)
    return json.loads(response.text)

def insert_result(result, meta):
    """ê²°ê³¼ ì‚½ì…"""
    table_id = f"{project_id}.{DATASET}.{RESULT_TABLE}"
    
    row = {
        "analysis_id": f"fmo-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}",
        "title_hash": meta['title_hash'],
        "published_at": meta['published_at'].isoformat() if hasattr(meta['published_at'], 'isoformat') else meta['published_at'],
        "observed_at": datetime.datetime.now().isoformat(),
        "delta_score": result['physics_engine']['module_1_delta']['kl_divergence'],
        "phase": result['physics_engine']['module_3_phase']['current_phase'],
        "analysis_payload": result,
        "strategic_narrative": result['fmo_output']['module_5_narratives']
    }
    
    # ì „ì—­ bq_client ì‚¬ìš©
    errors = bq_client.insert_rows_json(table_id, [row])
    return errors

# 4. ë©”ì¸ íŒŒì´í”„ë¼ì¸
def run_analyzer():
    # ë¯¸ë¶„ì„ ê¸°ì‚¬ ì¶”ì¶œ (NOT EXISTS ë¡œì§)
    query = f"""
    SELECT article_text, title_hash, published_at, title
    FROM `{project_id}.{DATASET}.{RAW_TABLE}` AS raw
    WHERE NOT EXISTS (
        SELECT 1 FROM `{project_id}.{DATASET}.{RESULT_TABLE}` AS res 
        WHERE res.title_hash = raw.title_hash
    )
    AND article_text IS NOT NULL
    ORDER BY published_at DESC LIMIT 5
    """
    
    rows = bq_client.query(query).result()
    
    for row in rows:
        try:
            print(f"ğŸ§¬ ë¶„ì„ ì¤‘: {row.title[:30]}...")
            analysis_res = analyze_article(row.article_text)
            
            meta = {"title_hash": row.title_hash, "published_at": row.published_at}
            errors = insert_result(analysis_res, meta)
            
            if not errors:
                print(f"âœ… ì„±ê³µ: {row.title_hash}")
            else:
                print(f"âŒ ì‚½ì… ì—ëŸ¬: {errors}")
        except Exception as e:
            print(f"âš ï¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    run_analyzer()
